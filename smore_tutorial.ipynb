{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwteQWjjAz3MqTt2f/vNMO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "Let's first import/install some dependent packages."
      ],
      "metadata": {
        "id": "I3Qxl3lJs6yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup the device\n",
        "\n",
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "if use_gpu:\n",
        "  device = 'cuda:0'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yj6dC9d_32vB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import basic modules\n",
        "\n",
        "import torch\n",
        "from google.colab import output\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "mH6CJ3y_a8j0",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependency\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install pickle5 pybind11\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "-u4R4BXFmCCM",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can install the smore package. As it contains highly optimized c++/cuda code, we need to compile it first. Below are two options to install the smore package. We will go with the first option to install a pre-build whl.\n",
        "\n",
        "If this doens't work (e.g., the machine is not compatible), you can try the second option to build from source, though it would be much slower."
      ],
      "metadata": {
        "id": "Kg3R-lz2n0yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Option 1: Install smore from pre-built whl\n",
        "!pip install https://snap.stanford.edu/logtutorial/smore-0.0.0-cp38-cp38-linux_x86_64.whl\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "bw6oIKcbR-jy",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Option 2: Install smore from source (If you really want to do so, you can uncomment the code below to run)\n",
        "# !pip install git+https://github.com/google-research/smore@wikikgv2\n",
        "# output.clear()"
      ],
      "metadata": {
        "id": "rNIJGuTC-May",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to import **smore**"
      ],
      "metadata": {
        "id": "gKSpF450Y2QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import smore"
      ],
      "metadata": {
        "id": "82Spsw7PJBIX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a small KG: FB15k"
      ],
      "metadata": {
        "id": "zUxsh_5cZErD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download a small KG from web, and unzip it."
      ],
      "metadata": {
        "id": "A8vtfkGDZWXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://snap.stanford.edu/logtutorial/FB15k-log.zip\"\n",
        "!unzip FB15k-log.zip\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "sYuSDB2AZVUo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **smore** package comes with a knowledge graph class that can be used to conveniently read/write/query large KGs. To enable that, we first import the class called **KGMem**, which stands for a shared memory object that holds the memory of KG."
      ],
      "metadata": {
        "id": "HtJ374vxZ7Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smore.cpp_sampler.sampler_clib import KGMem"
      ],
      "metadata": {
        "id": "lM85l3kNZ6Q0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It loads the pre-processed binary file into kgmem, and then we can create a KG object that references to this memory segment.\n",
        "\n",
        "** note that KgMem creates a KG without actually copying the memory, so one need to hold KgMem object during the lifetime of created KG."
      ],
      "metadata": {
        "id": "XaKLzDgx75ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_kg(kg_name):\n",
        "  kgmem = KGMem(dtype='uint32')\n",
        "  kgmem.load('%s/train_bidir.bin' % kg_name)\n",
        "  kg = kgmem.create_kg()\n",
        "  print('This KG has %d entities, %d edges on %d relations' % (\n",
        "      kg.num_ent, kg.num_edges, kg.num_rel))\n",
        "  return kgmem, kg\n",
        "\n",
        "kgmem, kg = load_kg('FB15k-log')"
      ],
      "metadata": {
        "id": "ArJWR_-Qf_ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73c40ea-bc94-495b-800d-10b0f388336b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This KG has 14951 entities, 966284 edges on 2690 relations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can use an exampler tuple of (head_entity, relation, tail_entity) to illustrate the query result in this KG."
      ],
      "metadata": {
        "id": "mX8LUlrLHG-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_entity = 834\n",
        "tail_entity = 546\n",
        "relation = 40\n",
        "\n",
        "print(f'KG has edge {head_entity} --{relation}--> {tail_entity} ?',\n",
        "      kg.has_forward_edge(head_entity, relation, tail_entity))\n",
        "\n",
        "print(f'KG has edge {tail_entity} <--{relation}-- {head_entity} ?',\n",
        "      kg.has_backward_edge(tail_entity, relation, head_entity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qefy93l__0EI",
        "outputId": "3c98b81c-2580-4c37-d13c-c3bd93f0ae42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KG has edge 834 --40--> 546 ? True\n",
            "KG has edge 546 <--40-- 834 ? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling queries from KG"
      ],
      "metadata": {
        "id": "hifqQvMqIbTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's import the sampler, which is responsible to generate positive/negative samples from the knowledge graph, according to different query templates."
      ],
      "metadata": {
        "id": "sT2VrOxMQEKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smore.cpp_sampler.online_sampler import OnlineSampler\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VJtobO4YOek7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then let's specify the hyperparameters of the sampler, and try a naive one first.\n",
        "\n",
        "This basic sample exhaustively explore the knowledge graph to obtain the positive and negative examples per query template."
      ],
      "metadata": {
        "id": "8KbDaVTiQPN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_template = '2p' #@param {type:\"string\"}\n",
        "num_negative_samples = 256 #@param {type:\"integer\"}\n",
        "sampler_type = 'naive' #@param {type:\"string\"}\n",
        "search_bandwidth = 14951 #@param {type:\"integer\"}\n",
        "max_intermediate_entities = 14951 #@param {type:\"integer\"}\n",
        "\n",
        "sample_mode = (search_bandwidth, 0, 'u', 'u', max_intermediate_entities)\n",
        "\n",
        "sampler = OnlineSampler(kg, [query_template], num_negative_samples, sample_mode,\n",
        "                        [1.0], sampler_type, share_negative=True,\n",
        "                        same_in_batch=True, num_threads=1)\n"
      ],
      "metadata": {
        "id": "c2pJPx5CSJpD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see it runs in a reasonable speed on this small KG.\n"
      ],
      "metadata": {
        "id": "9MK-lLiUS3YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10 #@param {type:\"integer\"}\n",
        "num_batches = 10000 #@param {type:\"integer\"}\n",
        "\n",
        "def test_sampler_speed(batch_gen):\n",
        "  cur_time = time.time()\n",
        "  for _ in tqdm(range(num_batches)):\n",
        "    next(batch_gen)\n",
        "  print('\\n')\n",
        "  total_time = time.time() - cur_time\n",
        "  ns = batch_size * num_batches\n",
        "  print(f'The {sampler_type} sampler takes {total_time} seconds for {ns} samples')\n",
        "\n",
        "batch_gen = sampler.batch_generator(batch_size)\n",
        "test_sampler_speed(batch_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HnBNlvxQCgW",
        "outputId": "1be35927-58fe-485b-fd1d-408de43de673"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1373.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The naive sampler takes 7.295474290847778 seconds for 100000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a more efficient sampler, where the theoretical cost is supposed to be a square-root of the naive one."
      ],
      "metadata": {
        "id": "xG8EOzaaUV9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler_type = 'sqrt'\n",
        "sampler = OnlineSampler(kg, [query_template], num_negative_samples, sample_mode,\n",
        "                        [1.0], sampler_type, share_negative=True,\n",
        "                        same_in_batch=True, num_threads=1)\n",
        "batch_gen = sampler.batch_generator(batch_size)\n",
        "test_sampler_speed(batch_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlt1SVkRUiNk",
        "outputId": "f1aa6009-fcec-4ba9-9fae-04d48a0e8a37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:04<00:00, 2305.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The sqrt sampler takes 4.350490093231201 seconds for 100000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see it indeed runs faster with the improved sampling technique, but the gain is not significant on small KGs.\n",
        "\n",
        "If one is interested you can try on larger KGs."
      ],
      "metadata": {
        "id": "UywzlLmHVO8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optional: run the above study on larger KGs.\n",
        "\n",
        "# !wget \"https://snap.stanford.edu/logtutorial/ogbl-wikikg2-log.zip\"\n",
        "# !unzip ogbl-wikikg2-log.zip\n",
        "# output.clear()\n",
        "# kgmem, kg = load_kg('ogbl-wikikg2-log')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-vVXpZKhVero",
        "outputId": "50dae9fe-0a7e-4b9b-80af-be9ca10eb663"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This KG has 2500604 entities, 32218364 edges on 1070 relations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before taking a look at the model interface, let's first understand the samples returned by the sampler:"
      ],
      "metadata": {
        "id": "v9KUSSQnvBl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_sample, negative_sample, is_negative_mat, subsampling_weight, batch_queries, query_structures = next(batch_gen)"
      ],
      "metadata": {
        "id": "9MmiQpKGfrVm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `batch_queries` is a batch of instantiations of the query template `2p`.\n",
        "\n",
        "In this case each query should be a tuple of (head_entity, relation-1, relation-2)"
      ],
      "metadata": {
        "id": "-eep9ijavtKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_queries[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPzR1syCvrE1",
        "outputId": "681577a7-286d-48cf-e79d-9a90b980629a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13240,   342,   522])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `is_negative_mat` matrix is a matrix of size (batch_size, num_negative_samples)\n",
        "\n",
        "where the entry(i, j) indicates whether entity j is a true negative example of sample i."
      ],
      "metadata": {
        "id": "JdM8YbJbwGum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build new models"
      ],
      "metadata": {
        "id": "OupnyAUewbKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A new model would inherit from a generic KGReasoning class"
      ],
      "metadata": {
        "id": "Ejry-WInw1lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smore.models.kg_reasoning import KGReasoning\n",
        "from smore.common.embedding.sparse_embed import SparseEmbedding\n",
        "\n",
        "\n",
        "class BasicReasoning(KGReasoning):\n",
        "\n",
        "  def __init__(self, nentity, nrelation, hidden_dim, gamma,\n",
        "               batch_size, test_batch_size=1, sparse_embeddings=None,\n",
        "               sparse_device='gpu', use_cuda=False, query_name_dict=None,\n",
        "               logit_impl='native'):\n",
        "    super(BasicReasoning, self).__init__(nentity=nentity, nrelation=nrelation, hidden_dim=hidden_dim,\n",
        "                                        gamma=gamma, optim_mode=None, batch_size=batch_size, test_batch_size=test_batch_size,\n",
        "                                        sparse_embeddings=sparse_embeddings, sparse_device=sparse_device, use_cuda=use_cuda,\n",
        "                                        query_name_dict=query_name_dict, logit_impl=logit_impl)\n",
        "\n",
        "    self.geo = 'basic'  # name of this module\n",
        "    self.entity_embedding = SparseEmbedding(nentity, self.entity_dim)  # suppose we need one embedding per each entity\n",
        "    self.num_embedding_component = 1  # and number of components in an embedding representation\n",
        "    self.init_params()  # finally let's initialize all the parameters\n",
        "\n",
        "  def relation_projection(self, cur_embedding, relation_ids):\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    relation_embedding = self.relation_embedding(relation_ids).unsqueeze(1)\n",
        "    return [cur_embedding[0] + relation_embedding]\n",
        "\n",
        "  def retrieve_embedding(self, entity_ids):\n",
        "    '''\n",
        "    Retrieve the entity embeddings given the entity indices\n",
        "    Args:\n",
        "        entity_ids: a list of entities indices\n",
        "    '''\n",
        "    embedding = self.entity_embedding(entity_ids)\n",
        "    return [embedding.unsqueeze(1)] # [num_queries, 1, embedding_dim]\n",
        "\n",
        "  def native_cal_logit(self, entity_embedding, entity_feat, query_embedding):\n",
        "    assert entity_feat is None\n",
        "    distance = entity_embedding.unsqueeze(1) - query_embedding[0]\n",
        "    logit = self.gamma - torch.norm(distance, p=1, dim=-1)\n",
        "    logit = torch.max(logit, dim=1)[0]\n",
        "    return logit\n"
      ],
      "metadata": {
        "id": "jd5-iUUuwFyW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicReasoning(nentity=kg.num_ent, nrelation=kg.num_rel,\n",
        "                       hidden_dim=128, gamma=1.0, batch_size=batch_size,\n",
        "                       query_name_dict={ ('e', ('r', 'r')): '2p'})"
      ],
      "metadata": {
        "id": "OYEw7BB5vpYs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_logit, negative_logit, _ = model(positive_sample,\n",
        "                                          negative_sample,\n",
        "                                          query_structures[0],\n",
        "                                          batch_queries,\n",
        "                                          device=device)"
      ],
      "metadata": {
        "id": "-i05ERmVgWTg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_logit = negative_logit * is_negative_mat\n",
        "negative_score = F.logsigmoid(-negative_logit).mean(dim=1)\n",
        "positive_score = F.logsigmoid(positive_logit).squeeze(dim=1)\n",
        "\n",
        "positive_sample_loss = -torch.mean(positive_score)\n",
        "negative_sample_loss = -torch.mean(negative_score)\n",
        "loss = positive_sample_loss + negative_sample_loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS4eqzCN3BBd",
        "outputId": "e736b316-d537-4b3e-acd7-6bd95a821f92"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1076, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}